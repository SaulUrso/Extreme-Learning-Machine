{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAG Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gettin the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datautils\n",
    "import pandas as pd\n",
    "import modelutils as mu\n",
    "import nesterov\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"Dataset/ML-CUP23-TR.csv\"\n",
    "RESULTS = \"Results/\"\n",
    "PLOT = \"Plots/\"\n",
    "RUNS = \"FullRuns/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cup = pd.read_csv(DATASET, skiprows=6)\n",
    "df_cup.rename(columns={\"# Training set: ID\": \"ID\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ID      1000 non-null   int64  \n",
      " 1   x1      1000 non-null   float64\n",
      " 2   x2      1000 non-null   float64\n",
      " 3   x3      1000 non-null   float64\n",
      " 4   x4      1000 non-null   float64\n",
      " 5   x5      1000 non-null   float64\n",
      " 6   x6      1000 non-null   float64\n",
      " 7   x7      1000 non-null   float64\n",
      " 8   x8      1000 non-null   float64\n",
      " 9   x9      1000 non-null   float64\n",
      " 10  x10     1000 non-null   float64\n",
      " 11  y1      1000 non-null   float64\n",
      " 12  y2      1000 non-null   float64\n",
      " 13  y3      1000 non-null   float64\n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 109.5 KB\n"
     ]
    }
   ],
   "source": [
    "df_cup.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10) (1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = datautils.obtain_features_targets(df_cup)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General experiments for algorithms comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioning of input weights for HIDDEN_SIZE 2000: 1.1187507619148291\n",
      "Conditioning of A for HIDDEN_SIZE 2000: 123776981.93134852\n",
      "0.9996312068571507\n"
     ]
    }
   ],
   "source": [
    "for HIDDEN_SIZE in [2000]:\n",
    "    model_true = mu.ELM(input_size, HIDDEN_SIZE, output_size, seed=1, init=\"fan-in\")\n",
    "    conditioning = np.linalg.cond(model_true.input_weights)\n",
    "    print(\n",
    "        f\"Conditioning of input weights for HIDDEN_SIZE {HIDDEN_SIZE}: {conditioning}\"\n",
    "    )\n",
    "\n",
    "    A = model_true.hidden_activations(X_train)\n",
    "    conditioning = np.linalg.cond(A)\n",
    "    print(f\"Conditioning of A for HIDDEN_SIZE {HIDDEN_SIZE}: {conditioning}\")\n",
    "\n",
    "    AtA = A.T @ A  # + alpha * np.eye(model.hidden_size)\n",
    "\n",
    "    # guarantee posdef in case of numerical errors (happens for large hidden sizes)\n",
    "    # NOTE: investigate what gradient descent does, however alpha propably guarantees always posdef\n",
    "    eig_min = np.min(np.linalg.eigvals(AtA))\n",
    "    old_tau = max(0, -eig_min)\n",
    "    BtB = AtA + (old_tau + 1e-2) * np.eye(model_true.hidden_size)\n",
    "\n",
    "    eigenvalues = np.linalg.eigvalsh(BtB)\n",
    "    L = np.max(eigenvalues)\n",
    "    tau = np.min(eigenvalues)\n",
    "\n",
    "    true_beta = (np.sqrt(L) - np.sqrt(tau)) / (np.sqrt(L) + np.sqrt(tau))\n",
    "\n",
    "    print(true_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    \"Hidden size\": [],\n",
    "    \"Mu\": [],\n",
    "    \"Beta\": [],\n",
    "    \"Epsilon\": [],\n",
    "    \"Alpha\": [],\n",
    "    \"Seed\": [],\n",
    "    \"Initialization type\": [],\n",
    "    \"Train\": [],\n",
    "    \"Validation\": [],\n",
    "    \"Time\": [],\n",
    "    \"Epochs\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# per 2000 1e-2 alpha\n",
    "# per 1000 1e-2 alpha\n",
    "# per 100 1e-2 alpha\n",
    "# per 50 1e-8 alpha\n",
    "\n",
    "hidden_size_list = [\n",
    "    2000,\n",
    "    50,\n",
    "    100,\n",
    "    1000,\n",
    "]\n",
    "lr_list = [\"optimal\"]\n",
    "beta_list = [0.9996263246315289, 0.9, 0.3, 0.6, 0.99, 0, \"schedule\"]\n",
    "alpha_list = [1e-2]\n",
    "epsilon_list = [np.float64(1e-10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN_SIZE: 2000, LEARNING_RATE: optimal, BETA: 0.9996263246315289, ALPHA: 0.01, EPSILON: 1e-10\n",
      "starting seed 0\n",
      "Optimal beta:  0.9996263246315289  Optimal lr:  3.4921368322023017e-06\n",
      "Training model using Nesterov accelerated gradient descent...\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1: \t train loss = 946.25555196, \tgrad norm = 524904.54643053\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 2: \t train loss = 337.45842840, \tgrad norm = 272409.34173895\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 3: \t train loss = 366.94827350, \tgrad norm = 69237.32338890\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 4: \t train loss = 409.45526650, \tgrad norm = 112330.92658862\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 5: \t train loss = 302.00008011, \tgrad norm = 147479.62388999\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 6: \t train loss = 203.99677173, \tgrad norm = 109977.51502841\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 7: \t train loss = 185.41485397, \tgrad norm = 49746.71434486\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 8: \t train loss = 195.23942118, \tgrad norm = 36585.24311030\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 9: \t train loss = 192.04460546, \tgrad norm = 54529.45381863\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 10: \t train loss = 179.85052903, \tgrad norm = 53469.32764982\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 11: \t train loss = 171.67356174, \tgrad norm = 41097.39517329\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 12: \t train loss = 169.28092920, \tgrad norm = 31464.54472022\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 13: \t train loss = 169.53446452, \tgrad norm = 29137.86397961\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 14: \t train loss = 171.15916889, \tgrad norm = 28544.28674752\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 15: \t train loss = 173.76146601, \tgrad norm = 27683.19615864\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 16: \t train loss = 175.87621578, \tgrad norm = 27975.70390142\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 17: \t train loss = 175.63656118, \tgrad norm = 29212.90685195\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 18: \t train loss = 172.13954470, \tgrad norm = 29877.34929465\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 19: \t train loss = 165.69355927, \tgrad norm = 29261.73773692\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 20: \t train loss = 157.21834949, \tgrad norm = 27661.78215643\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 21: \t train loss = 147.65099614, \tgrad norm = 25702.85009047\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 22: \t train loss = 137.68686698, \tgrad norm = 23868.39409355\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 23: \t train loss = 127.78340341, \tgrad norm = 22352.78022897\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 24: \t train loss = 118.29536430, \tgrad norm = 21081.69439728\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 25: \t train loss = 109.62689476, \tgrad norm = 19823.72116904\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 26: \t train loss = 102.29221965, \tgrad norm = 18369.58008750\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 27: \t train loss = 96.83013551, \tgrad norm = 16704.19114934\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 28: \t train loss = 93.61383279, \tgrad norm = 15095.26413106\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 29: \t train loss = 92.66675977, \tgrad norm = 14039.30575587\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 30: \t train loss = 93.59178226, \tgrad norm = 13971.15388784\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 31: \t train loss = 95.65794013, \tgrad norm = 14861.72446017\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 32: \t train loss = 98.01077425, \tgrad norm = 16220.98314389\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 33: \t train loss = 99.91892957, \tgrad norm = 17482.66129826\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 34: \t train loss = 100.96136496, \tgrad norm = 18256.29861392\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 35: \t train loss = 101.09281956, \tgrad norm = 18370.48605800\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 36: \t train loss = 100.57989311, \tgrad norm = 17844.06029921\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 37: \t train loss = 99.85001646, \tgrad norm = 16845.21041345\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 38: \t train loss = 99.32084493, \tgrad norm = 15643.74425893\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 39: \t train loss = 99.27221451, \tgrad norm = 14544.88291543\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 40: \t train loss = 99.79467087, \tgrad norm = 13794.05969180\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 41: \t train loss = 100.81338887, \tgrad norm = 13478.61908242\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 42: \t train loss = 102.15929335, \tgrad norm = 13503.83332916\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 43: \t train loss = 103.64901945, \tgrad norm = 13677.64561686\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 44: \t train loss = 105.14203876, \tgrad norm = 13825.96956405\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 45: \t train loss = 106.56022744, \tgrad norm = 13856.73516660\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 46: \t train loss = 107.87335621, \tgrad norm = 13765.90792394\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 47: \t train loss = 109.06619942, \tgrad norm = 13611.05559786\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 48: \t train loss = 110.10612146, \tgrad norm = 13471.86823390\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 49: \t train loss = 110.92531395, \tgrad norm = 13410.47545920\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 50: \t train loss = 111.42316949, \tgrad norm = 13444.53154135\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 51: \t train loss = 111.48582973, \tgrad norm = 13542.92500010\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 52: \t train loss = 111.01463264, \tgrad norm = 13642.24937038\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 53: \t train loss = 109.95387292, \tgrad norm = 13671.28132010\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 54: \t train loss = 108.31016445, \tgrad norm = 13571.00093883\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 55: \t train loss = 106.15915025, \tgrad norm = 13305.51864131\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 56: \t train loss = 103.63884677, \tgrad norm = 12865.60641296\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 57: \t train loss = 100.93165002, \tgrad norm = 12268.39001342\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 58: \t train loss = 98.23873780, \tgrad norm = 11555.89176787\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 59: \t train loss = 95.75142400, \tgrad norm = 10793.32278305\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 60: \t train loss = 93.62416797, \tgrad norm = 10065.79896798\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 61: \t train loss = 91.95351157, \tgrad norm = 9469.54141332\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 62: \t train loss = 90.76620436, \tgrad norm = 9092.49082409\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 63: \t train loss = 90.01821771, \tgrad norm = 8985.33822552\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 64: \t train loss = 89.60443740, \tgrad norm = 9138.72130880\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 65: \t train loss = 89.37693851, \tgrad norm = 9486.68353520\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 66: \t train loss = 89.16833342, \tgrad norm = 9934.41252112\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 67: \t train loss = 88.81609228, \tgrad norm = 10388.17382801\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 68: \t train loss = 88.18406640, \tgrad norm = 10772.54122208\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 69: \t train loss = 87.17851598, \tgrad norm = 11035.26838665\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 70: \t train loss = 85.75736164, \tgrad norm = 11145.48488332\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 71: \t train loss = 83.93270747, \tgrad norm = 11089.48470154\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 72: \t train loss = 81.76759313, \tgrad norm = 10866.27342766\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 73: \t train loss = 79.36831488, \tgrad norm = 10483.90727612\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 74: \t train loss = 76.87360845, \tgrad norm = 9957.17701775\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 75: \t train loss = 74.44174394, \tgrad norm = 9306.94622799\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 76: \t train loss = 72.23638219, \tgrad norm = 8561.28386498\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 77: \t train loss = 70.41202760, \tgrad norm = 7758.41024590\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 78: \t train loss = 69.10008129, \tgrad norm = 6951.27374052\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 79: \t train loss = 68.39672998, \tgrad norm = 6212.58452532\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 80: \t train loss = 68.35402120, \tgrad norm = 5635.40038066\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 81: \t train loss = 68.97533234, \tgrad norm = 5316.94473317\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 82: \t train loss = 70.21598684, \tgrad norm = 5315.87320397\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 83: \t train loss = 71.98907056, \tgrad norm = 5611.04400391\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 84: \t train loss = 74.17570660, \tgrad norm = 6112.85454964\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 85: \t train loss = 76.63834735, \tgrad norm = 6713.32365562\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 86: \t train loss = 79.23520131, \tgrad norm = 7321.68782581\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 87: \t train loss = 81.83382279, \tgrad norm = 7873.59354779\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 88: \t train loss = 84.32215680, \tgrad norm = 8328.32971091\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 89: \t train loss = 86.61587151, \tgrad norm = 8664.02089480\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 90: \t train loss = 88.66149359, \tgrad norm = 8873.57856931\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 91: \t train loss = 90.43554368, \tgrad norm = 8961.52934411\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 92: \t train loss = 91.94042406, \tgrad norm = 8941.34826440\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 93: \t train loss = 93.19815752, \tgrad norm = 8832.99683067\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 94: \t train loss = 94.24318782, \tgrad norm = 8660.49942426\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 95: \t train loss = 95.11535058, \tgrad norm = 8449.50026201\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 96: \t train loss = 95.85386524, \tgrad norm = 8224.83981089\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 97: \t train loss = 96.49286099, \tgrad norm = 8008.29586836\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 98: \t train loss = 97.05860605, \tgrad norm = 7816.74199096\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 99: \t train loss = 97.56832229, \tgrad norm = 7661.03125899\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 100: \t train loss = 98.03027152, \tgrad norm = 7545.84161134\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 101: \t train loss = 98.44470895, \tgrad norm = 7470.49604964\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 102: \t train loss = 98.80530065, \tgrad norm = 7430.48493549\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 103: \t train loss = 99.10067193, \tgrad norm = 7419.22786081\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 104: \t train loss = 99.31585803, \tgrad norm = 7429.62118601\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 105: \t train loss = 99.43354063, \tgrad norm = 7455.08979578\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 106: \t train loss = 99.43504943, \tgrad norm = 7490.07795720\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 107: \t train loss = 99.30117681, \tgrad norm = 7530.07317300\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 108: \t train loss = 99.01289181, \tgrad norm = 7571.32655537\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 109: \t train loss = 98.55205077, \tgrad norm = 7610.43356523\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 110: \t train loss = 97.90219093, \tgrad norm = 7643.90506123\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 111: \t train loss = 97.04946686, \tgrad norm = 7667.81754059\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 112: \t train loss = 95.98375057, \tgrad norm = 7677.59661192\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 113: \t train loss = 94.69987024, \tgrad norm = 7667.96199950\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 114: \t train loss = 93.19891229, \tgrad norm = 7633.04335936\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 115: \t train loss = 91.48946351, \tgrad norm = 7566.66062617\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 116: \t train loss = 89.58863240, \tgrad norm = 7462.74891322\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 117: \t train loss = 87.52266906, \tgrad norm = 7315.89682556\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 118: \t train loss = 85.32701038, \tgrad norm = 7121.96049195\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 119: \t train loss = 83.04561409, \tgrad norm = 6878.71577058\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 120: \t train loss = 80.72951253, \tgrad norm = 6586.51888277\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 121: \t train loss = 78.43460616, \tgrad norm = 6248.96013259\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 122: \t train loss = 76.21881664, \tgrad norm = 5873.51189716\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 123: \t train loss = 74.13881366, \tgrad norm = 5472.17859228\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 124: \t train loss = 72.24660348, \tgrad norm = 5062.12227950\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 125: \t train loss = 70.58630681, \tgrad norm = 4666.09444773\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 126: \t train loss = 69.19145204, \tgrad norm = 4312.13899143\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 127: \t train loss = 68.08306620, \tgrad norm = 4031.41054590\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 128: \t train loss = 67.26876482, \tgrad norm = 3852.62623109\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 129: \t train loss = 66.74293618, \tgrad norm = 3793.27916798\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 130: \t train loss = 66.48799718, \tgrad norm = 3851.95826987\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 131: \t train loss = 66.47658506, \tgrad norm = 4008.04883483\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 132: \t train loss = 66.67445297, \tgrad norm = 4229.40106933\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 133: \t train loss = 67.04377024, \tgrad norm = 4481.86724386\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 134: \t train loss = 67.54649559, \tgrad norm = 4735.52897259\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 135: \t train loss = 68.14749554, \tgrad norm = 4967.06563729\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 136: \t train loss = 68.81711821, \tgrad norm = 5159.91778313\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 137: \t train loss = 69.53299904, \tgrad norm = 5303.67144122\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 138: \t train loss = 70.28096091, \tgrad norm = 5393.34842416\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 139: \t train loss = 71.05496727, \tgrad norm = 5428.81604924\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 140: \t train loss = 71.85618328, \tgrad norm = 5414.32574530\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 141: \t train loss = 72.69128667, \tgrad norm = 5358.11351174\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 142: \t train loss = 73.57023970, \tgrad norm = 5271.95666945\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 143: \t train loss = 74.50377895, \tgrad norm = 5170.54458618\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 144: \t train loss = 75.50089795, \tgrad norm = 5070.48867865\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 145: \t train loss = 76.56658734, \tgrad norm = 4988.81071234\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 146: \t train loss = 77.70006001, \tgrad norm = 4940.88283363\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 147: \t train loss = 78.89362964, \tgrad norm = 4938.09410075\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 148: \t train loss = 80.13233669, \tgrad norm = 4985.87755189\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 149: \t train loss = 81.39433486, \tgrad norm = 5082.84409463\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 150: \t train loss = 82.65197230, \tgrad norm = 5221.39672068\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 151: \t train loss = 83.87343371, \tgrad norm = 5389.53651942\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 152: \t train loss = 85.02475997, \tgrad norm = 5573.13031348\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 153: \t train loss = 86.07203398, \tgrad norm = 5757.95359661\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 154: \t train loss = 86.98351900, \tgrad norm = 5931.16174225\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 155: \t train loss = 87.73155587, \tgrad norm = 6082.16067308\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 156: \t train loss = 88.29406524, \tgrad norm = 6203.00463856\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 157: \t train loss = 88.65555398, \tgrad norm = 6288.47288799\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 158: \t train loss = 88.80758409, \tgrad norm = 6335.94325633\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 159: \t train loss = 88.74872036, \tgrad norm = 6345.13869984\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 160: \t train loss = 88.48402292, \tgrad norm = 6317.79160684\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 161: \t train loss = 88.02418799, \tgrad norm = 6257.25265991\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 162: \t train loss = 87.38446111, \tgrad norm = 6168.06316132\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 163: \t train loss = 86.58345163, \tgrad norm = 6055.50863795\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 164: \t train loss = 85.64196599, \tgrad norm = 5925.17454720\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 165: \t train loss = 84.58195474, \tgrad norm = 5782.52980142\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 166: \t train loss = 83.42563710, \tgrad norm = 5632.56829663\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 167: \t train loss = 82.19483347, \tgrad norm = 5479.53998824\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 168: \t train loss = 80.91050454, \tgrad norm = 5326.79865219\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 169: \t train loss = 79.59246988, \tgrad norm = 5176.78173372\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 170: \t train loss = 78.25926158, \tgrad norm = 5031.11934115\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 171: \t train loss = 76.92806128, \tgrad norm = 4890.84798065\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 172: \t train loss = 75.61467121, \tgrad norm = 4756.68537602\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 173: \t train loss = 74.33348012, \tgrad norm = 4629.31075560\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 174: \t train loss = 73.09740092, \tgrad norm = 4509.59319411\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 175: \t train loss = 71.91777470, \tgrad norm = 4398.71910288\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 176: \t train loss = 70.80425287, \tgrad norm = 4298.18716335\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 177: \t train loss = 69.76468207, \tgrad norm = 4209.66298113\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 178: \t train loss = 68.80502354, \tgrad norm = 4134.71453791\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 179: \t train loss = 67.92933864, \tgrad norm = 4074.47963828\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 180: \t train loss = 67.13986517, \tgrad norm = 4029.34101131\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 181: \t train loss = 66.43719665, \tgrad norm = 3998.69353307\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 182: \t train loss = 65.82056096, \tgrad norm = 3980.87308232\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 183: \t train loss = 65.28817735, \tgrad norm = 3973.27875523\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 184: \t train loss = 64.83765657, \tgrad norm = 3972.67229533\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 185: \t train loss = 64.46639778, \tgrad norm = 3975.59903049\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 186: \t train loss = 64.17193214, \tgrad norm = 3978.85619321\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 187: \t train loss = 63.95216548, \tgrad norm = 3979.93780433\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 188: \t train loss = 63.80548233, \tgrad norm = 3977.40111740\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 189: \t train loss = 63.73068953, \tgrad norm = 3971.11672194\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 190: \t train loss = 63.72679684, \tgrad norm = 3962.37583618\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 191: \t train loss = 63.79265327, \tgrad norm = 3953.83348521\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 192: \t train loss = 63.92647733, \tgrad norm = 3949.27011066\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 193: \t train loss = 64.12533530, \tgrad norm = 3953.16517883\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 194: \t train loss = 64.38463126, \tgrad norm = 3970.10301852\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 195: \t train loss = 64.69767548, \tgrad norm = 4004.07542114\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 196: \t train loss = 65.05539249, \tgrad norm = 4057.79534608\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 197: \t train loss = 65.44621769, \tgrad norm = 4132.16434338\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 198: \t train loss = 65.85621356, \tgrad norm = 4226.01588286\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 199: \t train loss = 66.26941414, \tgrad norm = 4336.18501245\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 200: \t train loss = 66.66838343, \tgrad norm = 4457.86339612\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 201: \t train loss = 67.03495096, \tgrad norm = 4585.13281982\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 202: \t train loss = 67.35106928, \tgrad norm = 4711.55445432\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 203: \t train loss = 67.59972512, \tgrad norm = 4830.71745752\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 204: \t train loss = 67.76582982, \tgrad norm = 4936.69316620\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 205: \t train loss = 67.83701574, \tgrad norm = 5024.37820775\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 206: \t train loss = 67.80427404, \tgrad norm = 5089.73238719\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 207: \t train loss = 67.66238360, \tgrad norm = 5129.92634134\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 208: \t train loss = 67.41010024, \tgrad norm = 5143.41465766\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 209: \t train loss = 67.05009689, \tgrad norm = 5129.94698556\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 210: \t train loss = 66.58866755, \tgrad norm = 5090.52544298\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 211: \t train loss = 66.03522778, \tgrad norm = 5027.31267321\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 212: \t train loss = 65.40166107, \tgrad norm = 4943.49175745\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 213: \t train loss = 64.70157113, \tgrad norm = 4843.07714245\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 214: \t train loss = 63.94950560, \tgrad norm = 4730.67528912\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 215: \t train loss = 63.16021474, \tgrad norm = 4611.19573809\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 216: \t train loss = 62.34800214, \tgrad norm = 4489.51881082\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 217: \t train loss = 61.52621158, \tgrad norm = 4370.13595976\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 218: \t train loss = 60.70687929, \tgrad norm = 4256.79213492\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 219: \t train loss = 59.90056285, \tgrad norm = 4152.17307583\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 220: \t train loss = 59.11634091, \tgrad norm = 4057.68797123\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 221: \t train loss = 58.36196200, \tgrad norm = 3973.39263511\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 222: \t train loss = 57.64410819, \tgrad norm = 3898.07725654\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 223: \t train loss = 56.96873043, \tgrad norm = 3829.51060484\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 224: \t train loss = 56.34140909, \tgrad norm = 3764.80087149\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 225: \t train loss = 55.76769356, \tgrad norm = 3700.81423641\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 226: \t train loss = 55.25338040, \tgrad norm = 3634.59130425\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 227: \t train loss = 54.80469812, \tgrad norm = 3563.71542191\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 228: \t train loss = 54.42837811, \tgrad norm = 3486.60690823\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 229: \t train loss = 54.13160361, \tgrad norm = 3402.73495743\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 230: \t train loss = 53.92184129, \tgrad norm = 3312.74947282\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 231: \t train loss = 53.80657109, \tgrad norm = 3218.53699316\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 232: \t train loss = 53.79293899, \tgrad norm = 3123.19902270\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 233: \t train loss = 53.88736374, \tgrad norm = 3030.93958904\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 234: \t train loss = 54.09513125, \tgrad norm = 2946.83593058\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 235: \t train loss = 54.42001019, \tgrad norm = 2876.46038141\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 236: \t train loss = 54.86391896, \tgrad norm = 2825.33677203\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 237: \t train loss = 55.42666876, \tgrad norm = 2798.26456114\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 238: \t train loss = 56.10579998, \tgrad norm = 2798.62380513\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 239: \t train loss = 56.89652111, \tgrad norm = 2827.84238195\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 240: \t train loss = 57.79175149, \tgrad norm = 2885.19907503\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 241: \t train loss = 58.78226122, \tgrad norm = 2968.02603051\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 242: \t train loss = 59.85689613, \tgrad norm = 3072.22136628\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 243: \t train loss = 61.00287057, \tgrad norm = 3192.88793586\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 244: \t train loss = 62.20610861, \tgrad norm = 3324.92328386\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 245: \t train loss = 63.45161296, \tgrad norm = 3463.46172028\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 246: \t train loss = 64.72384205, \tgrad norm = 3604.14859664\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 247: \t train loss = 66.00707740, \tgrad norm = 3743.27408033\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 248: \t train loss = 67.28576681, \tgrad norm = 3877.80838676\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 249: \t train loss = 68.54483180, \tgrad norm = 4005.37643541\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 250: \t train loss = 69.76993204, \tgrad norm = 4124.19974058\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 251: \t train loss = 70.94768222, \tgrad norm = 4233.02361107\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 252: \t train loss = 72.06582054, \tgrad norm = 4331.04051195\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 253: \t train loss = 73.11332988, \tgrad norm = 4417.81569654\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 254: \t train loss = 74.08051497, \tgrad norm = 4493.21827153\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 255: \t train loss = 74.95904021, \tgrad norm = 4557.35905697\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 256: \t train loss = 75.74193347, \tgrad norm = 4610.53548793\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 257: \t train loss = 76.42356179, \tgrad norm = 4653.18311211\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 258: \t train loss = 76.99958519, \tgrad norm = 4685.83283779\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 259: \t train loss = 77.46689443, \tgrad norm = 4709.07292447\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 260: \t train loss = 77.82353896, \tgrad norm = 4723.51476276\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 261: \t train loss = 78.06865028, \tgrad norm = 4729.76173687\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 262: \t train loss = 78.20236606, \tgrad norm = 4728.38086797\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 263: \t train loss = 78.22575946, \tgrad norm = 4719.87743643\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 264: \t train loss = 78.14077707, \tgrad norm = 4704.67329723\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 265: \t train loss = 77.95018821, \tgrad norm = 4683.09004157\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 266: \t train loss = 77.65754678, \tgrad norm = 4655.33843162\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 267: \t train loss = 77.26716527, \tgrad norm = 4621.51557651\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 268: \t train loss = 76.78409950, \tgrad norm = 4581.61109552\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 269: \t train loss = 76.21414060, \tgrad norm = 4535.52304379\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 270: \t train loss = 75.56380985, \tgrad norm = 4483.08371534\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 271: \t train loss = 74.84035078, \tgrad norm = 4424.09468155\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 272: \t train loss = 74.05171241, \tgrad norm = 4358.36967361\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 273: \t train loss = 73.20651756, \tgrad norm = 4285.78327188\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 274: \t train loss = 72.31401062, \tgrad norm = 4206.32288503\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 275: \t train loss = 71.38398045, \tgrad norm = 4120.14120964\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 276: \t train loss = 70.42665556, \tgrad norm = 4027.60622709\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 277: \t train loss = 69.45257105, \tgrad norm = 3929.34575328\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 278: \t train loss = 68.47240890, \tgrad norm = 3826.28350985\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 279: \t train loss = 67.49681559, \tgrad norm = 3719.66353016\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 280: \t train loss = 66.53620328, \tgrad norm = 3611.05936670\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 281: \t train loss = 65.60054251, \tgrad norm = 3502.36401600\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 282: \t train loss = 64.69915546, \tgrad norm = 3395.75585148\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 283: \t train loss = 63.84051985, \tgrad norm = 3293.63552219\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 284: \t train loss = 63.03209280, \tgrad norm = 3198.52942596\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 285: \t train loss = 62.28016362, \tgrad norm = 3112.95798216\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 286: \t train loss = 61.58974258, \tgrad norm = 3039.27246512\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 287: \t train loss = 60.96449096, \tgrad norm = 2979.47279662\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 288: \t train loss = 60.40669484, \tgrad norm = 2935.02883281\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 289: \t train loss = 59.91728292, \tgrad norm = 2906.73543255\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 290: \t train loss = 59.49588582, \tgrad norm = 2894.63192668\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 291: \t train loss = 59.14093206, \tgrad norm = 2898.00640082\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 292: \t train loss = 58.84977413, \tgrad norm = 2915.48640884\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 293: \t train loss = 58.61883651, \tgrad norm = 2945.19747224\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 294: \t train loss = 58.44377717, \tgrad norm = 2984.95727869\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 295: \t train loss = 58.31965350, \tgrad norm = 3032.47120682\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 296: \t train loss = 58.24108476, \tgrad norm = 3085.50212645\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 297: \t train loss = 58.20240384, \tgrad norm = 3141.99916757\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 298: \t train loss = 58.19779282, \tgrad norm = 3200.18122187\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 299: \t train loss = 58.22139880, \tgrad norm = 3258.57858393\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 300: \t train loss = 58.26742797, \tgrad norm = 3316.04000604\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 301: \t train loss = 58.33021845, \tgrad norm = 3371.71351227\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 302: \t train loss = 58.40429327, \tgrad norm = 3425.00884643\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 303: \t train loss = 58.48439721, \tgrad norm = 3475.54837575\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 304: \t train loss = 58.56552115, \tgrad norm = 3523.11217430\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 305: \t train loss = 58.64291913, \tgrad norm = 3567.58207114\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 306: \t train loss = 58.71212247, \tgrad norm = 3608.88866804\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 307: \t train loss = 58.76895543, \tgrad norm = 3646.96463035\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 308: \t train loss = 58.80955604, \tgrad norm = 3681.70684182\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 309: \t train loss = 58.83040444, \tgrad norm = 3712.94923485\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 310: \t train loss = 58.82835999, \tgrad norm = 3740.44725781\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 311: \t train loss = 58.80070706, \tgrad norm = 3763.87406373\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 312: \t train loss = 58.74520797, \tgrad norm = 3782.82767315\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 313: \t train loss = 58.66016061, \tgrad norm = 3796.84765966\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 314: \t train loss = 58.54445705, \tgrad norm = 3805.43939284\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 315: \t train loss = 58.39763912, \tgrad norm = 3808.10358577\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 316: \t train loss = 58.21994633, \tgrad norm = 3804.36882725\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 317: \t train loss = 58.01235195, \tgrad norm = 3793.82490150\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 318: \t train loss = 57.77658304, \tgrad norm = 3776.15495451\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 319: \t train loss = 57.51512125, \tgrad norm = 3751.16489772\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 320: \t train loss = 57.23118210, \tgrad norm = 3718.80878937\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 321: \t train loss = 56.92867155, \tgrad norm = 3679.20925876\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 322: \t train loss = 56.61211990, \tgrad norm = 3632.67230695\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 323: \t train loss = 56.28659455, \tgrad norm = 3579.69601407\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 324: \t train loss = 55.95759381, \tgrad norm = 3520.97280296\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 325: \t train loss = 55.63092576, \tgrad norm = 3457.38495821\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 326: \t train loss = 55.31257604, \tgrad norm = 3389.99309663\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 327: \t train loss = 55.00856995, \tgrad norm = 3320.01725932\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 328: \t train loss = 54.72483356, \tgrad norm = 3248.81029494\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 329: \t train loss = 54.46705932, \tgrad norm = 3177.82329579\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 330: \t train loss = 54.24058052, \tgrad norm = 3108.56311635\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 331: \t train loss = 54.05025907, \tgrad norm = 3042.54253907\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 332: \t train loss = 53.90038949, \tgrad norm = 2981.22451651\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 333: \t train loss = 53.79462181, \tgrad norm = 2925.96309821\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 334: \t train loss = 53.73590430, \tgrad norm = 2877.94499694\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 335: \t train loss = 53.72644637, \tgrad norm = 2838.13694267\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 336: \t train loss = 53.76770087, \tgrad norm = 2807.24456351\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 337: \t train loss = 53.86036393, \tgrad norm = 2785.68807547\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 338: \t train loss = 54.00439027, \tgrad norm = 2773.59833641\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 339: \t train loss = 54.19902092, \tgrad norm = 2770.83401368\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 340: \t train loss = 54.44282052, \tgrad norm = 2777.01736113\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 341: \t train loss = 54.73372111, \tgrad norm = 2791.58326458\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 342: \t train loss = 55.06906975, \tgrad norm = 2813.83454710\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 343: \t train loss = 55.44567754, \tgrad norm = 2842.99636566\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 344: \t train loss = 55.85986835, \tgrad norm = 2878.26371872\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 345: \t train loss = 56.30752628, \tgrad norm = 2918.83810949\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 346: \t train loss = 56.78414114, \tgrad norm = 2963.95166556\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 347: \t train loss = 57.28485241, \tgrad norm = 3012.87901525\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 348: \t train loss = 57.80449234, \tgrad norm = 3064.93869208\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 349: \t train loss = 58.33762925, \tgrad norm = 3119.48671010\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 350: \t train loss = 58.87861247, \tgrad norm = 3175.90528385\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 351: \t train loss = 59.42162031, \tgrad norm = 3233.58957673\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 352: \t train loss = 59.96071242, \tgrad norm = 3291.93498026\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 353: \t train loss = 60.48988735, \tgrad norm = 3350.32687139\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 354: \t train loss = 61.00314599, \tgrad norm = 3408.13416188\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 355: \t train loss = 61.49456092, \tgrad norm = 3464.70732123\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 356: \t train loss = 61.95835090, \tgrad norm = 3519.38098071\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 357: \t train loss = 62.38895966, \tgrad norm = 3571.48075262\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 358: \t train loss = 62.78113696, \tgrad norm = 3620.33354795\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 359: \t train loss = 63.13002008, \tgrad norm = 3665.28045397\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 360: \t train loss = 63.43121316, \tgrad norm = 3705.69113294\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 361: \t train loss = 63.68086183, \tgrad norm = 3740.97870520\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 362: \t train loss = 63.87572064, \tgrad norm = 3770.61416125\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 363: \t train loss = 64.01321068, \tgrad norm = 3794.13948329\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 364: \t train loss = 64.09146560, \tgrad norm = 3811.17882298\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 365: \t train loss = 64.10936409, \tgrad norm = 3821.44726162\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 366: \t train loss = 64.06654782, \tgrad norm = 3824.75685573\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 367: \t train loss = 63.96342441, \tgrad norm = 3821.01983700\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 368: \t train loss = 63.80115521, \tgrad norm = 3810.24898391\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 369: \t train loss = 63.58162890, \tgrad norm = 3792.55531126\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 370: \t train loss = 63.30742167, \tgrad norm = 3768.14333103\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 371: \t train loss = 62.98174593, \tgrad norm = 3737.30422565\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 372: \t train loss = 62.60838921, \tgrad norm = 3700.40734099\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 373: \t train loss = 62.19164548, \tgrad norm = 3657.89045421\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 374: \t train loss = 61.73624104, \tgrad norm = 3610.24929956\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 375: \t train loss = 61.24725728, \tgrad norm = 3558.02684418\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 376: \t train loss = 60.73005220, \tgrad norm = 3501.80279509\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 377: \t train loss = 60.19018252, \tgrad norm = 3442.18378708\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 378: \t train loss = 59.63332812, \tgrad norm = 3379.79464788\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 379: \t train loss = 59.06521976, \tgrad norm = 3315.27106155\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 380: \t train loss = 58.49157110, \tgrad norm = 3249.25385202\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 381: \t train loss = 57.91801563, \tgrad norm = 3182.38498765\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 382: \t train loss = 57.35004864, \tgrad norm = 3115.30526520\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 383: \t train loss = 56.79297431, \tgrad norm = 3048.65347218\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 384: \t train loss = 56.25185775, \tgrad norm = 2983.06665562\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 385: \t train loss = 55.73148167, \tgrad norm = 2919.18095266\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 386: \t train loss = 55.23630722, \tgrad norm = 2857.63227828\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 387: \t train loss = 54.77043885, \tgrad norm = 2799.05603634\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 388: \t train loss = 54.33759260, \tgrad norm = 2744.08494923\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 389: \t train loss = 53.94106783, \tgrad norm = 2693.34411837\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 390: \t train loss = 53.58372225, \tgrad norm = 2647.44256766\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 391: \t train loss = 53.26795024, \tgrad norm = 2606.96081283\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 392: \t train loss = 52.99566482, \tgrad norm = 2572.43445396\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 393: \t train loss = 52.76828359, \tgrad norm = 2544.33438543\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 394: \t train loss = 52.58671902, \tgrad norm = 2523.04488998\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 395: \t train loss = 52.45137381, \tgrad norm = 2508.84151477\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 396: \t train loss = 52.36214165, \tgrad norm = 2501.87106533\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 397: \t train loss = 52.31841416, \tgrad norm = 2502.13615368\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 398: \t train loss = 52.31909424, \tgrad norm = 2509.48641266\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 399: \t train loss = 52.36261619, \tgrad norm = 2523.61776043\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 400: \t train loss = 52.44697282, \tgrad norm = 2544.08010232\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 401: \t train loss = 52.56974929, \tgrad norm = 2570.29280935\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 402: \t train loss = 52.72816342, \tgrad norm = 2601.56643927\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 403: \t train loss = 52.91911194, \tgrad norm = 2637.12862714\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 404: \t train loss = 53.13922165, \tgrad norm = 2676.15192133\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 405: \t train loss = 53.38490475, \tgrad norm = 2717.78152607\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 406: \t train loss = 53.65241684, \tgrad norm = 2761.16131603\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 407: \t train loss = 53.93791640, \tgrad norm = 2805.45698181\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 408: \t train loss = 54.23752434, \tgrad norm = 2849.87564013\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 409: \t train loss = 54.54738211, \tgrad norm = 2893.68163331\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 410: \t train loss = 54.86370712, \tgrad norm = 2936.20852769\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 411: \t train loss = 55.18284422, \tgrad norm = 2976.86750340\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 412: \t train loss = 55.50131211, \tgrad norm = 3015.15243187\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 413: \t train loss = 55.81584395, \tgrad norm = 3050.64198726\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 414: \t train loss = 56.12342153, \tgrad norm = 3082.99915666\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 415: \t train loss = 56.42130266, \tgrad norm = 3111.96851904\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 416: \t train loss = 56.70704176, \tgrad norm = 3137.37166375\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 417: \t train loss = 56.97850376, \tgrad norm = 3159.10112268\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 418: \t train loss = 57.23387181, \tgrad norm = 3177.11319427\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 419: \t train loss = 57.47164921, \tgrad norm = 3191.42004284\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 420: \t train loss = 57.69065626, \tgrad norm = 3202.08145726\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 421: \t train loss = 57.89002292, \tgrad norm = 3209.19664607\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 422: \t train loss = 58.06917776, \tgrad norm = 3212.89642702\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 423: \t train loss = 58.22783413, \tgrad norm = 3213.33613480\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 424: \t train loss = 58.36597408, \tgrad norm = 3210.68951994\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 425: \t train loss = 58.48383056, \tgrad norm = 3205.14384459\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 426: \t train loss = 58.58186838, \tgrad norm = 3196.89629940\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 427: \t train loss = 58.66076411, \tgrad norm = 3186.15177388\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 428: \t train loss = 58.72138517, \tgrad norm = 3173.12191601\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 429: \t train loss = 58.76476815, \tgrad norm = 3158.02532196\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 430: \t train loss = 58.79209637, \tgrad norm = 3141.08860999\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 431: \t train loss = 58.80467659, \tgrad norm = 3122.54806116\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 432: \t train loss = 58.80391496, \tgrad norm = 3102.65145818\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 433: \t train loss = 58.79129205, \tgrad norm = 3081.65972706\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 434: \t train loss = 58.76833711, \tgrad norm = 3059.84798757\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 435: \t train loss = 58.73660176, \tgrad norm = 3037.50564837\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 436: \t train loss = 58.69763318, \tgrad norm = 3014.93524152\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 437: \t train loss = 58.65294733, \tgrad norm = 2992.44977652\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 438: \t train loss = 58.60400255, \tgrad norm = 2970.36850441\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 439: \t train loss = 58.55217408, \tgrad norm = 2949.01111072\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 440: \t train loss = 58.49873013, \tgrad norm = 2928.69049678\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 441: \t train loss = 58.44481000, \tgrad norm = 2909.70445182\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 442: \t train loss = 58.39140503, \tgrad norm = 2892.32665042\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 443: \t train loss = 58.33934277, \tgrad norm = 2876.79751802\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 444: \t train loss = 58.28927485, \tgrad norm = 2863.31557580\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 445: \t train loss = 58.24166910, \tgrad norm = 2852.02989260\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 446: \t train loss = 58.19680594, \tgrad norm = 2843.03422692\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 447: \t train loss = 58.15477922, \tgrad norm = 2836.36333614\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 448: \t train loss = 58.11550148, \tgrad norm = 2831.99177012\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 449: \t train loss = 58.07871337, \tgrad norm = 2829.83526991\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 450: \t train loss = 58.04399679, \tgrad norm = 2829.75468244\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 451: \t train loss = 58.01079152, \tgrad norm = 2831.56210486\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 452: \t train loss = 57.97841441, \tgrad norm = 2835.02881131\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 453: \t train loss = 57.94608085, \tgrad norm = 2839.89440726\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 454: \t train loss = 57.91292756, \tgrad norm = 2845.87661009\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 455: \t train loss = 57.87803623, \tgrad norm = 2852.68106718\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 456: \t train loss = 57.84045716, \tgrad norm = 2860.01068623\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 457: \t train loss = 57.79923254, \tgrad norm = 2867.57405063\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 458: \t train loss = 57.75341859, \tgrad norm = 2875.09261164\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 459: \t train loss = 57.70210630, \tgrad norm = 2882.30647238\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 460: \t train loss = 57.64444042, \tgrad norm = 2888.97869573\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 461: \t train loss = 57.57963629, \tgrad norm = 2894.89816967\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 462: \t train loss = 57.50699458, \tgrad norm = 2899.88114604\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 463: \t train loss = 57.42591378, \tgrad norm = 2903.77162911\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 464: \t train loss = 57.33590037, \tgrad norm = 2906.44082991\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 465: \t train loss = 57.23657687, \tgrad norm = 2907.78592240\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 466: \t train loss = 57.12768783, \tgrad norm = 2907.72834004\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 467: \t train loss = 57.00910386, \tgrad norm = 2906.21183981\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 468: \t train loss = 56.88082381, \tgrad norm = 2903.20053704\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 469: \t train loss = 56.74297535, \tgrad norm = 2898.67708134\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 470: \t train loss = 56.59581393, \tgrad norm = 2892.64110471\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 471: \t train loss = 56.43972026, \tgrad norm = 2885.10802926\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 472: \t train loss = 56.27519641, \tgrad norm = 2876.10827675\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 473: \t train loss = 56.10286053, \tgrad norm = 2865.68687816\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 474: \t train loss = 55.92344027, \tgrad norm = 2853.90343993\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 475: \t train loss = 55.73776480, \tgrad norm = 2840.83238723\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 476: \t train loss = 55.54675562, \tgrad norm = 2826.56337529\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 477: \t train loss = 55.35141603, \tgrad norm = 2811.20173756\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 478: \t train loss = 55.15281946, \tgrad norm = 2794.86882748\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 479: \t train loss = 54.95209658, \tgrad norm = 2777.70210668\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 480: \t train loss = 54.75042144, \tgrad norm = 2759.85483940\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 481: \t train loss = 54.54899670, \tgrad norm = 2741.49526886\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 482: \t train loss = 54.34903827, \tgrad norm = 2722.80517651\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 483: \t train loss = 54.15175947, \tgrad norm = 2703.97775900\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 484: \t train loss = 53.95835508, \tgrad norm = 2685.21479864\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 485: \t train loss = 53.76998558, \tgrad norm = 2666.72315007\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 486: \t train loss = 53.58776189, \tgrad norm = 2648.71061603\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 487: \t train loss = 53.41273089, \tgrad norm = 2631.38133615\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 488: \t train loss = 53.24586221, \tgrad norm = 2614.93086090\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 489: \t train loss = 53.08803634, \tgrad norm = 2599.54112355\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 490: \t train loss = 52.94003459, \tgrad norm = 2585.37555264\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 491: \t train loss = 52.80253088, \tgrad norm = 2572.57458138\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 492: \t train loss = 52.67608569, \tgrad norm = 2561.25180502\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 493: \t train loss = 52.56114216, \tgrad norm = 2551.49101146\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 494: \t train loss = 52.45802443, \tgrad norm = 2543.34426463\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 495: \t train loss = 52.36693803, \tgrad norm = 2536.83115660\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 496: \t train loss = 52.28797252, \tgrad norm = 2531.93926992\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 497: \t train loss = 52.22110578, \tgrad norm = 2528.62581203\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 498: \t train loss = 52.16621016, \tgrad norm = 2526.82030793\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 499: \t train loss = 52.12305990, \tgrad norm = 2526.42817227\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 500: \t train loss = 52.09133975, \tgrad norm = 2527.33493474\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 501: \t train loss = 52.07065434, \tgrad norm = 2529.41086574\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 502: \t train loss = 52.06053821, \tgrad norm = 2532.51574507\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 503: \t train loss = 52.06046593, \tgrad norm = 2536.50353152\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 504: \t train loss = 52.06986234, \tgrad norm = 2541.22672411\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 505: \t train loss = 52.08811246, \tgrad norm = 2546.54024985\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 506: \t train loss = 52.11457097, \tgrad norm = 2552.30476323\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 507: \t train loss = 52.14857116, \tgrad norm = 2558.38929507\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 508: \t train loss = 52.18943308, \tgrad norm = 2564.67323682\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 509: \t train loss = 52.23647089, \tgrad norm = 2571.04768930\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 510: \t train loss = 52.28899949, \tgrad norm = 2577.41623836\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 511: \t train loss = 52.34634013, \tgrad norm = 2583.69524463\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 512: \t train loss = 52.40782524, \tgrad norm = 2589.81374796\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 513: \t train loss = 52.47280249, \tgrad norm = 2595.71309203\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 514: \t train loss = 52.54063787, \tgrad norm = 2601.34637013\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 515: \t train loss = 52.61071822, \tgrad norm = 2606.67778164\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 516: \t train loss = 52.68245285, \tgrad norm = 2611.68197166\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 517: \t train loss = 52.75527456, \tgrad norm = 2616.34340510\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 518: \t train loss = 52.82863995, \tgrad norm = 2620.65580430\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 519: \t train loss = 52.90202914, \tgrad norm = 2624.62165581\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 520: \t train loss = 52.97494482, \tgrad norm = 2628.25177160\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 521: \t train loss = 53.04691074, \tgrad norm = 2631.56487182\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 522: \t train loss = 53.11746964, \tgrad norm = 2634.58714322\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 523: \t train loss = 53.18618067, \tgrad norm = 2637.35171971\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 524: \t train loss = 53.25261638, \tgrad norm = 2639.89802966\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 525: \t train loss = 53.31635921, \tgrad norm = 2642.27095873\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 526: \t train loss = 53.37699781, \tgrad norm = 2644.51978752\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 527: \t train loss = 53.43412312, \tgrad norm = 2646.69687771\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 528: \t train loss = 53.48732429, \tgrad norm = 2648.85610006\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 529: \t train loss = 53.53618485, \tgrad norm = 2651.05101856\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 530: \t train loss = 53.58027894, \tgrad norm = 2653.33286796\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 531: \t train loss = 53.61916807, \tgrad norm = 2655.74838287\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 532: \t train loss = 53.65239836, \tgrad norm = 2658.33755601\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 533: \t train loss = 53.67949862, \tgrad norm = 2661.13141719\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 534: \t train loss = 53.69997916, \tgrad norm = 2664.14993349\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 535: \t train loss = 53.71333184, \tgrad norm = 2667.40013291\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 536: \t train loss = 53.71903106, \tgrad norm = 2670.87454851\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 537: \t train loss = 53.71653608, \tgrad norm = 2674.55006759\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 538: \t train loss = 53.70529458, \tgrad norm = 2678.38725212\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 539: \t train loss = 53.68474748, \tgrad norm = 2682.33017326\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 540: \t train loss = 53.65433498, \tgrad norm = 2686.30677651\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 541: \t train loss = 53.61350377, \tgrad norm = 2690.22976752\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 542: \t train loss = 53.56171525, \tgrad norm = 2693.99798244\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 543: \t train loss = 53.49845470, \tgrad norm = 2697.49818500\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 544: \t train loss = 53.42324111, \tgrad norm = 2700.60721481\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 545: \t train loss = 53.33563754, \tgrad norm = 2703.19439969\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 546: \t train loss = 53.23526176, \tgrad norm = 2705.12414000\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 547: \t train loss = 53.12179701, \tgrad norm = 2706.25857311\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 548: \t train loss = 52.99500250, \tgrad norm = 2706.46023287\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 549: \t train loss = 52.85472358, \tgrad norm = 2705.59462925\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 550: \t train loss = 52.70090130, \tgrad norm = 2703.53268754\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 551: \t train loss = 52.53358112, \tgrad norm = 2700.15300177\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 552: \t train loss = 52.35292062, \tgrad norm = 2695.34387414\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 553: \t train loss = 52.15919602, \tgrad norm = 2689.00512734\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 554: \t train loss = 51.95280748, \tgrad norm = 2681.04969189\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 555: \t train loss = 51.73428282, \tgrad norm = 2671.40498236\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 556: \t train loss = 51.50427983, \tgrad norm = 2660.01408626\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 557: \t train loss = 51.26358691, \tgrad norm = 2646.83679582\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 558: \t train loss = 51.01312212, \tgrad norm = 2631.85051643\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 559: \t train loss = 50.75393046, \tgrad norm = 2615.05108592\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 560: \t train loss = 50.48717957, \tgrad norm = 2596.45353619\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 561: \t train loss = 50.21415362, \tgrad norm = 2576.09282351\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 562: \t train loss = 49.93624565, \tgrad norm = 2554.02454600\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 563: \t train loss = 49.65494822, \tgrad norm = 2530.32565687\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 564: \t train loss = 49.37184254, \tgrad norm = 2505.09517041\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 565: \t train loss = 49.08858612, \tgrad norm = 2478.45484431\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 566: \t train loss = 48.80689898, \tgrad norm = 2450.54980723\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 567: \t train loss = 48.52854866, \tgrad norm = 2421.54908528\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 568: \t train loss = 48.25533404, \tgrad norm = 2391.64596437\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 569: \t train loss = 47.98906821, \tgrad norm = 2361.05810926\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 570: \t train loss = 47.73156047, \tgrad norm = 2330.02734346\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 571: \t train loss = 47.48459783, \tgrad norm = 2298.81897964\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 572: \t train loss = 47.24992593, \tgrad norm = 2267.72057781\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 573: \t train loss = 47.02922999, \tgrad norm = 2237.04000112\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 574: \t train loss = 46.82411565, \tgrad norm = 2207.10263962\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 575: \t train loss = 46.63609026, \tgrad norm = 2178.24768375\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 576: \t train loss = 46.46654473, \tgrad norm = 2150.82335566\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 577: \t train loss = 46.31673619, \tgrad norm = 2125.18105222\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 578: \t train loss = 46.18777185, \tgrad norm = 2101.66842038\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 579: \t train loss = 46.08059419, \tgrad norm = 2080.62147376\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 580: \t train loss = 45.99596775, \tgrad norm = 2062.35596611\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 581: \t train loss = 45.93446781, \tgrad norm = 2047.15835098\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 582: \t train loss = 45.89647106, \tgrad norm = 2035.27676595\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 583: \t train loss = 45.88214848, \tgrad norm = 2026.91256196\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 584: \t train loss = 45.89146054, \tgrad norm = 2022.21293618\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 585: \t train loss = 45.92415488, \tgrad norm = 2021.26520292\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 586: \t train loss = 45.97976643, \tgrad norm = 2024.09314487\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 587: \t train loss = 46.05762005, \tgrad norm = 2030.65573263\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 588: \t train loss = 46.15683569, \tgrad norm = 2040.84830315\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 589: \t train loss = 46.27633597, \tgrad norm = 2054.50607770\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 590: \t train loss = 46.41485615, \tgrad norm = 2071.40970865\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 591: \t train loss = 46.57095624, \tgrad norm = 2091.29240172\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 592: \t train loss = 46.74303523, \tgrad norm = 2113.84808114\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 593: \t train loss = 46.92934723, \tgrad norm = 2138.74005422\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 594: \t train loss = 47.12801925, \tgrad norm = 2165.60967951\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 595: \t train loss = 47.33707037, \tgrad norm = 2194.08463101\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 596: \t train loss = 47.55443224, \tgrad norm = 2223.78646059\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 597: \t train loss = 47.77797042, \tgrad norm = 2254.33727283\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 598: \t train loss = 48.00550643, \tgrad norm = 2285.36542718\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 599: \t train loss = 48.23484026, \tgrad norm = 2316.51026412\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 600: \t train loss = 48.46377297, \tgrad norm = 2347.42591097\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 601: \t train loss = 48.69012916, \tgrad norm = 2377.78426131\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 602: \t train loss = 48.91177907, \tgrad norm = 2407.27724124\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 603: \t train loss = 49.12666006, \tgrad norm = 2435.61848149\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 604: \t train loss = 49.33279710, \tgrad norm = 2462.54450987\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 605: \t train loss = 49.52832217, \tgrad norm = 2487.81556758\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 606: \t train loss = 49.71149235, \tgrad norm = 2511.21613886\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 607: \t train loss = 49.88070620, \tgrad norm = 2532.55526764\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 608: \t train loss = 50.03451850, \tgrad norm = 2551.66671973\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 609: \t train loss = 50.17165303, \tgrad norm = 2568.40903416\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 610: \t train loss = 50.29101320, \tgrad norm = 2582.66549502\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 611: \t train loss = 50.39169062, \tgrad norm = 2594.34404358\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 612: \t train loss = 50.47297125, \tgrad norm = 2603.37714135\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 613: \t train loss = 50.53433931, \tgrad norm = 2609.72158722\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 614: \t train loss = 50.57547875, \tgrad norm = 2613.35828569\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 615: \t train loss = 50.59627237, \tgrad norm = 2614.29195855\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 616: \t train loss = 50.59679861, \tgrad norm = 2612.55078924\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 617: \t train loss = 50.57732603, \tgrad norm = 2608.18598630\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 618: \t train loss = 50.53830572, \tgrad norm = 2601.27125094\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 619: \t train loss = 50.48036150, \tgrad norm = 2591.90213289\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 620: \t train loss = 50.40427845, \tgrad norm = 2580.19525798\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 621: \t train loss = 50.31098957, \tgrad norm = 2566.28741089\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 622: \t train loss = 50.20156108, \tgrad norm = 2550.33445686\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 623: \t train loss = 50.07717643, \tgrad norm = 2532.51008623\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 624: \t train loss = 49.93911935, \tgrad norm = 2513.00436670\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 625: \t train loss = 49.78875607, \tgrad norm = 2492.02208911\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 626: \t train loss = 49.62751723, \tgrad norm = 2469.78089398\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 627: \t train loss = 49.45687943, \tgrad norm = 2446.50916860\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 628: \t train loss = 49.27834696, \tgrad norm = 2422.44370746\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 629: \t train loss = 49.09343384, \tgrad norm = 2397.82713373\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 630: \t train loss = 48.90364639, \tgrad norm = 2372.90508585\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 631: \t train loss = 48.71046669, \tgrad norm = 2347.92318160\n",
      "computing momentum\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 632: \t train loss = 48.51533701, \tgrad norm = 2323.12378283\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     42\u001b[39m start = time.process_time()\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[32m     45\u001b[39m (\n\u001b[32m     46\u001b[39m     final_model,\n\u001b[32m     47\u001b[39m     loss_train_history,\n\u001b[32m     48\u001b[39m     sol_dist_history,\n\u001b[32m     49\u001b[39m     grad_history,\n\u001b[32m     50\u001b[39m     epochs,\n\u001b[32m     51\u001b[39m     exec_time_loop,\n\u001b[32m     52\u001b[39m     has_problem,\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m ) = \u001b[43mnesterov\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALPHA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBETA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we are sure unless of numerical errors of the convergence, just here to prevent runs spanning hours\u001b[39;49;00m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPSILON\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexact_solution\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_true\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# fast_mode=True,\u001b[39;49;00m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m end = time.process_time()\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_problem:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Notebooks/CM/MLProj19Cm2324/nesterov.py:152\u001b[39m, in \u001b[36mnag\u001b[39m\u001b[34m(model, X, Y, lr, alpha, beta, max_epochs, eps, prec_error, exact_solution, verbose, check_float64, fast_mode)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m update_grad.dtype == np.float64:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mupdate_grad must be of type np.float64\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m:  \u001b[38;5;66;03m# exact line search\u001b[39;00m\n\u001b[32m    153\u001b[39m     stepsize = np.linalg.norm(update_grad, \u001b[33m\"\u001b[39m\u001b[33mfro\u001b[39m\u001b[33m\"\u001b[39m) ** \u001b[32m2\u001b[39m / (\n\u001b[32m    154\u001b[39m         np.trace(update_grad.T @ BtB @ update_grad) + prec_error\n\u001b[32m    155\u001b[39m     )\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m lr == \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# exact line search on all columns\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "worked = None\n",
    "for HIDDEN_SIZE, LEARNING_RATE, BETA, ALPHA, EPSILON in itertools.product(\n",
    "    hidden_size_list, lr_list, beta_list, alpha_list, epsilon_list\n",
    "):\n",
    "    print(\n",
    "        f\"HIDDEN_SIZE: {HIDDEN_SIZE}, LEARNING_RATE: {LEARNING_RATE}, BETA: {BETA}, ALPHA: {ALPHA}, EPSILON: {EPSILON}\"\n",
    "    )\n",
    "    # initialize dictionary for results\n",
    "    results_dict = {\n",
    "        \"Hidden size\": [],\n",
    "        \"Mu\": [],\n",
    "        \"Beta\": [],\n",
    "        \"Epsilon\": [],\n",
    "        \"Alpha\": [],\n",
    "        \"Seed\": [],\n",
    "        \"Initialization type\": [],\n",
    "        \"Train\": [],\n",
    "        \"Time\": [],\n",
    "        \"Time int\": [],\n",
    "        \"Epochs\": [],\n",
    "    }\n",
    "\n",
    "    # if worked is not None and not type(LEARNING_RATE) == str:\n",
    "    #     if (\n",
    "    #         LEARNING_RATE < worked\n",
    "    #     ):  # an higher learning rate has already been tested, and it converged\n",
    "    #         continue\n",
    "\n",
    "    # loop over seeds\n",
    "    for seed in range(3):\n",
    "\n",
    "        # compute exact solution using qr which has guaranteed better numerical stability\n",
    "        model_true = mu.ELM(input_size, HIDDEN_SIZE, output_size, seed=seed)\n",
    "        model_true.compute_wout_system_qr(X_train, y_train, alpha=ALPHA)\n",
    "\n",
    "        # initialize model\n",
    "        model = mu.ELM(input_size, HIDDEN_SIZE, output_size, seed=seed)\n",
    "\n",
    "        print(\"starting seed\", seed)\n",
    "\n",
    "        # measure time\n",
    "        start = time.process_time()\n",
    "\n",
    "        # train model\n",
    "        (\n",
    "            final_model,\n",
    "            loss_train_history,\n",
    "            sol_dist_history,\n",
    "            grad_history,\n",
    "            epochs,\n",
    "            exec_time_loop,\n",
    "            has_problem,\n",
    "        ) = nesterov.nag(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            lr=LEARNING_RATE,\n",
    "            alpha=ALPHA,\n",
    "            beta=BETA,\n",
    "            max_epochs=10000000,  # we are sure unless of numerical errors of the convergence, just here to prevent runs spanning hours\n",
    "            eps=EPSILON,\n",
    "            exact_solution=model_true.output_weights,\n",
    "            # fast_mode=True,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        end = time.process_time()\n",
    "\n",
    "        if has_problem:\n",
    "            print(\n",
    "                f\"Problem with ELM {HIDDEN_SIZE} {LEARNING_RATE} {BETA} {ALPHA} {EPSILON} {seed}\"\n",
    "            )\n",
    "            continue\n",
    "        else:\n",
    "            worked = LEARNING_RATE\n",
    "\n",
    "        # save results\n",
    "        results_dict[\"Hidden size\"].append(HIDDEN_SIZE)\n",
    "        results_dict[\"Mu\"].append(LEARNING_RATE)\n",
    "        results_dict[\"Beta\"].append(BETA)\n",
    "        results_dict[\"Alpha\"].append(ALPHA)\n",
    "        results_dict[\"Epsilon\"].append(EPSILON)\n",
    "        results_dict[\"Seed\"].append(seed)\n",
    "        results_dict[\"Initialization type\"].append(\"fan-in\")\n",
    "        results_dict[\"Train\"].append(loss_train_history[-1])\n",
    "        results_dict[\"Time\"].append(end - start)\n",
    "        results_dict[\"Time int\"].append(exec_time_loop)\n",
    "        results_dict[\"Epochs\"].append(epochs)\n",
    "\n",
    "        # guard is true if not using fast_mode (i.e. all iterations are saved)\n",
    "        if len(loss_train_history) > 1:\n",
    "            # save all iterations using pickle\n",
    "            with open(\n",
    "                f\"{RUNS}ELM_{HIDDEN_SIZE}_{LEARNING_RATE}_{BETA}_{ALPHA}_{EPSILON}_{seed}.pkl\",\n",
    "                \"wb\",\n",
    "            ) as f:\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        \"model\": final_model,\n",
    "                        \"loss_train_history\": loss_train_history,\n",
    "                        \"sol_dist_history\": sol_dist_history,\n",
    "                        \"grad_history\": grad_history,\n",
    "                    },\n",
    "                    f,\n",
    "                )\n",
    "\n",
    "    if has_problem:\n",
    "        continue\n",
    "\n",
    "    # no results obtained because there was an error\n",
    "    if len(results_dict[\"Seed\"]) == 0:\n",
    "        continue\n",
    "\n",
    "    df_results = pd.DataFrame(results_dict)\n",
    "    df_results.to_csv(\n",
    "        f\"{RESULTS}ELM_{HIDDEN_SIZE}_{LEARNING_RATE}_{BETA}_{ALPHA}_{EPSILON}-trialnew1.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size_list = [50, 100, 1000, 2000]\n",
    "lr_list = [\n",
    "    \"col\",\n",
    "    \"optimal\",\n",
    "    1e-4,\n",
    "    1e-5,\n",
    "    1e-6,\n",
    "    1e-7,\n",
    "]\n",
    "beta_list = [\"optimal\"]\n",
    "alpha_list = [1e-2]\n",
    "epsilon_list = [np.float64(1e-12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN_SIZE: 50, LEARNING_RATE: col, BETA: optimal, ALPHA: 0.01, EPSILON: 1e-12\n",
      "starting seed 0\n"
     ]
    }
   ],
   "source": [
    "worked = None\n",
    "for HIDDEN_SIZE, LEARNING_RATE, BETA, ALPHA, EPSILON in itertools.product(\n",
    "    hidden_size_list, lr_list, beta_list, alpha_list, epsilon_list\n",
    "):\n",
    "    print(\n",
    "        f\"HIDDEN_SIZE: {HIDDEN_SIZE}, LEARNING_RATE: {LEARNING_RATE}, BETA: {BETA}, ALPHA: {ALPHA}, EPSILON: {EPSILON}\"\n",
    "    )\n",
    "    # initialize dictionary for results\n",
    "    results_dict = {\n",
    "        \"Hidden size\": [],\n",
    "        \"Mu\": [],\n",
    "        \"Beta\": [],\n",
    "        \"Epsilon\": [],\n",
    "        \"Alpha\": [],\n",
    "        \"Seed\": [],\n",
    "        \"Initialization type\": [],\n",
    "        \"Train\": [],\n",
    "        \"Time\": [],\n",
    "        \"Time int\": [],\n",
    "        \"Epochs\": [],\n",
    "    }\n",
    "\n",
    "    # if worked is not None and not type(LEARNING_RATE) == str:\n",
    "    #     if (\n",
    "    #         LEARNING_RATE < worked\n",
    "    #     ):  # an higher learning rate has already been tested, and it converged\n",
    "    #         continue\n",
    "\n",
    "    # loop over seeds\n",
    "    for seed in range(3):\n",
    "\n",
    "        # compute exact solution using qr which has guaranteed better numerical stability\n",
    "        model_true = mu.ELM(input_size, HIDDEN_SIZE, output_size, seed=seed)\n",
    "        model_true.compute_wout_system_qr(X_train, y_train, alpha=ALPHA)\n",
    "\n",
    "        # initialize model\n",
    "        model = mu.ELM(input_size, HIDDEN_SIZE, output_size, seed=seed)\n",
    "\n",
    "        print(\"starting seed\", seed)\n",
    "\n",
    "        # measure time\n",
    "        start = time.process_time()\n",
    "\n",
    "        # train model\n",
    "        (\n",
    "            final_model,\n",
    "            loss_train_history,\n",
    "            sol_dist_history,\n",
    "            grad_history,\n",
    "            epochs,\n",
    "            exec_time_loop,\n",
    "            has_problem,\n",
    "        ) = nesterov.nag(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            lr=LEARNING_RATE,\n",
    "            alpha=ALPHA,\n",
    "            beta=BETA,\n",
    "            max_epochs=10000000,  # we are sure unless of numerical errors of the convergence, just here to prevent runs spanning hours\n",
    "            eps=EPSILON,\n",
    "            exact_solution=model_true.output_weights,\n",
    "            # fast_mode=True,\n",
    "            # verbose=True,\n",
    "        )\n",
    "\n",
    "        end = time.process_time()\n",
    "\n",
    "        if has_problem:\n",
    "            print(\n",
    "                f\"Problem with ELM {HIDDEN_SIZE} {LEARNING_RATE} {BETA} {ALPHA} {EPSILON} {seed}\"\n",
    "            )\n",
    "            continue\n",
    "        else:\n",
    "            worked = LEARNING_RATE\n",
    "\n",
    "        # save results\n",
    "        results_dict[\"Hidden size\"].append(HIDDEN_SIZE)\n",
    "        results_dict[\"Mu\"].append(LEARNING_RATE)\n",
    "        results_dict[\"Beta\"].append(BETA)\n",
    "        results_dict[\"Alpha\"].append(ALPHA)\n",
    "        results_dict[\"Epsilon\"].append(EPSILON)\n",
    "        results_dict[\"Seed\"].append(seed)\n",
    "        results_dict[\"Initialization type\"].append(\"fan-in\")\n",
    "        results_dict[\"Train\"].append(loss_train_history[-1])\n",
    "        results_dict[\"Time\"].append(end - start)\n",
    "        results_dict[\"Time int\"].append(exec_time_loop)\n",
    "        results_dict[\"Epochs\"].append(epochs)\n",
    "\n",
    "        # guard is true if not using fast_mode (i.e. all iterations are saved)\n",
    "        if len(loss_train_history) > 1:\n",
    "            # save all iterations using pickle\n",
    "            with open(\n",
    "                f\"{RUNS}ELM_{HIDDEN_SIZE}_{LEARNING_RATE}_{BETA}_{ALPHA}_{EPSILON}_{seed}.pkl\",\n",
    "                \"wb\",\n",
    "            ) as f:\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        \"model\": final_model,\n",
    "                        \"loss_train_history\": loss_train_history,\n",
    "                        \"sol_dist_history\": sol_dist_history,\n",
    "                        \"grad_history\": grad_history,\n",
    "                    },\n",
    "                    f,\n",
    "                )\n",
    "\n",
    "    if has_problem:\n",
    "        continue\n",
    "\n",
    "    # no results obtained because there was an error\n",
    "    if len(results_dict[\"Seed\"]) == 0:\n",
    "        continue\n",
    "\n",
    "    df_results = pd.DataFrame(results_dict)\n",
    "    df_results.to_csv(\n",
    "        f\"{RESULTS}ELM_{HIDDEN_SIZE}_{LEARNING_RATE}_{BETA}_{ALPHA}_{EPSILON}-trialnew1.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(\n",
    "    np.arange(1, len(loss_train_history) + 1),\n",
    "    loss_train_history\n",
    "    - min(\n",
    "        loss_train_history\n",
    "    ),  # mu.compute_loss(y_train, model_true.predict(X_train), alpha=ALPHA),\n",
    ")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(\n",
    "    np.arange(1, len(sol_dist_history) + 1),\n",
    "    sol_dist_history,\n",
    "    label=\"Train\",\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "# plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(\n",
    "    np.arange(1, len(grad_history)),\n",
    "    grad_history[1:],\n",
    "    label=\"Train\",\n",
    ")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from results get dataframes with 100 hidden, auto lr, schedule beta, 1e-2 epsilon, for every alpha\n",
    "alphas = [3e-1, 1e-1, 1e-2]\n",
    "res_list = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    res_list.append(\n",
    "        pd.read_csv(f\"{RESULTS}ELM_2000_auto_schedule_{alpha}_0.01-prime.csv\")\n",
    "    )\n",
    "\n",
    "# concatenate dataframes\n",
    "df_results = pd.concat(res_list, ignore_index=True)\n",
    "df_results.groupby(\"Alpha\")[\"Validation\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments for initialization comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for results, with keys the strings and values empty lists\n",
    "results_dict = {\n",
    "    \"Hidden size\": [],\n",
    "    \"Mu\": [],\n",
    "    \"Beta\": [],\n",
    "    \"Epsilon\": [],\n",
    "    \"Alpha\": [],\n",
    "    \"Seed\": [],\n",
    "    \"Initialization type\": [],\n",
    "    \"Train\": [],\n",
    "    \"Validation\": [],\n",
    "    \"Time\": [],\n",
    "    \"Epochs\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "HIDDEN_SIZE = 1000\n",
    "LEARNING_RATE = \"auto\"\n",
    "BETA = 0\n",
    "ALPHA = 1e-8\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over multiple seeds\n",
    "\n",
    "for initialization in [\"fan-in\", \"std\"]:\n",
    "    for seed in range(10):\n",
    "        # initialize the model\n",
    "        model = mu.ELM(\n",
    "            input_size,\n",
    "            HIDDEN_SIZE,\n",
    "            output_size,\n",
    "            seed=seed,\n",
    "            init=initialization,\n",
    "        )\n",
    "\n",
    "        # measure the time\n",
    "        start_time = time.process_time()\n",
    "\n",
    "        # train model\n",
    "        final_model, loss_train_history, loss_val_history, epochs, _ = nesterov.nag(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            lr=LEARNING_RATE,\n",
    "            alpha=ALPHA,\n",
    "            beta=BETA,\n",
    "            max_epochs=1000000,\n",
    "            eps=EPSILON,\n",
    "            # fast_mode=True,\n",
    "            # verbose=True,\n",
    "        )\n",
    "\n",
    "        end_time = time.process_time()\n",
    "\n",
    "        # save the results\n",
    "\n",
    "        results_dict[\"Hidden size\"].append(HIDDEN_SIZE)\n",
    "        results_dict[\"Mu\"].append(LEARNING_RATE)\n",
    "        results_dict[\"Beta\"].append(BETA)\n",
    "        results_dict[\"Epsilon\"].append(EPSILON)\n",
    "        results_dict[\"Alpha\"].append(ALPHA)\n",
    "        results_dict[\"Seed\"].append(seed)\n",
    "        results_dict[\"Initialization type\"].append(initialization)\n",
    "        results_dict[\"Train\"].append(loss_train_history[-1])\n",
    "        results_dict[\"Validation\"].append(loss_val_history[-1])\n",
    "        results_dict[\"Time\"].append(end_time - start_time)\n",
    "        results_dict[\"Epochs\"].append(epochs)\n",
    "\n",
    "        # save all iterations using pickle\n",
    "        with open(\n",
    "            f\"{initialization}_{seed}.pkl\",\n",
    "            \"wb\",\n",
    "        ) as f:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"model\": final_model,\n",
    "                    \"loss_train_history\": loss_train_history,\n",
    "                    \"loss_val_history\": loss_val_history,\n",
    "                },\n",
    "                f,\n",
    "            )\n",
    "\n",
    "        # plot results\n",
    "        plt.plot(\n",
    "            np.arange(1, len(loss_train_history) + 1),\n",
    "            loss_train_history,\n",
    "            label=\"Train\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            np.arange(1, len(loss_train_history) + 1),\n",
    "            loss_val_history,\n",
    "            label=\"Validation\",\n",
    "        )\n",
    "        plt.legend()\n",
    "        # set title and labels\n",
    "        plt.title(f\"Loss {initialization} {seed}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        # set axis scale\n",
    "        plt.xscale(\"log\")\n",
    "        plt.yscale(\"log\")\n",
    "        # fix y axis to have comparable plots\n",
    "        plt.ylim(0.001, 1000)\n",
    "\n",
    "        # save the plot\n",
    "        plt.savefig(PLOT + f\"loss_{initialization}_{seed}.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "results = pd.DataFrame(results_dict)\n",
    "results.to_csv(RESULTS + f\"results_initialization_{HIDDEN_SIZE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show average training, validation and time, show also the standard deviation\n",
    "results.groupby(\"Initialization type\")[[\"Train\", \"Validation\", \"Time\"]].agg(\n",
    "    [\"mean\", \"std\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
